\documentclass[10pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=2cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hyperref,booktabs}
\usepackage{float}
\linespread{1}
\usepackage{caption,subcaption}
\usepackage{minted}
\usepackage[T1]{fontenc}
\usepackage{fontawesome}
\usepackage{float}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{titlesec}
\emergencystretch=1em

\usepackage[style=authoryear-comp,maxbibnames=9,maxcitenames=1,uniquelist=false,backend=biber]{biblatex}
\addbibresource{references.bib}
% \DeclareNameAlias{default}{family-given}


\hypersetup{%
  colorlinks=true,
  citecolor=purple,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}

\setlength{\parindent}{1em}
\setlength{\parskip}{.2em}

\titlespacing*{\section}{0pt}{3ex}{1ex}
\titlespacing*{\subsection}
{0pt}{3ex}{3ex}


\title{\huge \textbf{RPE Proposal}}
\date{}
\author{Rohit Murali \and Supervisor: Cristina Conati}
\begin{document}

\maketitle

\section{Background and Motivation}

Emotions play a large part in learning. Studies \autocite{baker2010better, wortha2019multiple} show that emotions can affect learning in Intelligent Tutoring Systems (ITSs). This encourages educators to design ITSs which can adapt to a student's emotions during their interaction \autocite{woolf2009affect, grawemeyer2016affecting}. These systems require detecting the user's affective state to an extent during learning. There is work in ITS that tries to predict emotion valence \autocite{lalle2018prediction, salmeron2014evaluation} and single emotion \autocite{jaques2014predicting, sims2020neural, woolf2009affect, paquette2014sensor, sabourin2011modeling, lalle2016predicting}, but there is to the best of our knowledge, only one work that predicts co-occurring emotions \autocite{lalle2021predict}.
There is work that shows that emotions can co-occur simultaneously \autocite{bosch2014co, dillon2016student, harley2012measuring, gutica2013student, sinclair2018changes}. Thus, predicting co-occurring emotions is an important step towards developing affect-aware ITSs that adapt to a student experiencing multiple emotions. This RPE project will focus on this task.
We look specifically at the ITS, MetaTutor \autocite{azevedo2013using}, that delivers content about the circulatory system via text and diagrams, and includes mechanisms to support Self-Regulated Learning (SRL). Prediction tasks with MetaTutor are done by collecting students' eye-tracking and interaction data. During the interaction with MetaTutor, users' gaze data are tracked with an eye-tracker. Users are asked regularly to report if they felt any of the Pekrunâ€™s emotions \autocite{pekrun2014self}, by completing an Emotions and Value (EV) Questionnaire \autocite{azevedo2013using}. These listed an item of the form ``Right now I feel X'' for each emotion (e.g., ``Right now I feel bored''). These EV reports serve as the ground truth for prediction tasks.

\cite{jaques2014predicting} predicts boredom and curiosity in a MetaTutor study using eye-tracking data. The eye-tracker used is the Tobii T60. The dataset (dataset A onward) has valid eye-tracking data for 61 students with 270 EV reports and valid interaction data for 65 students with 325 EV reports. In this study students were asked 5 EVs each at regular intervals of 14 minutes.
\cite{lalle2021predict} looks at predicting pairs of co-occurring emotions using eye-tracking and interaction data with MetaTutor. This study uses Dataset A as well. This work was done as part of my RA project that started in Winter Term 1 2020. This work contributes to research in developing affect-aware ITSs in the following ways. First, we provide more evidence on the presence of co-occurring emotions during learning. Second, we show the feasibility of standard machine-learning models in predicting when emotions co-occur in MetaTutor. Third, our predictive models leverage both interaction and eye-tracking data. These two data sources have shown promising results for affect detection when used in isolation, but they have never been compared and/or combined, thus our results provide novel insights of the value of these data sources for affect detection. We also found that simple feature fusion of the two types of data did not improve performance over models trained on individual data sources.

In \cite{lalle2021predict}, we found evidence for two or more co-occurring emotions in over 80 \% of EV reports. We focused on mixed or negative valence pairs and chose the following pairs for prediction: Boredom+Frustration (Bo+Fr), and Anxiety+Curiosity (An+Cu). We restrict our prediction tasks to pairs of co-occurring emotions as higher-order classification tasks might still be difficult considering the limited size of the dataset. Thus, we look at four-way classifications. For an emotion pair, the four classes are None, First emotion alone, Second emotion alone, Both. For the pair An+Cu, we found that eye-tracking data was the best for predicting the classes None, Curiosity and Both, whereas interaction data could best predict Anxiety alone. For the pair Bo+Fr, we found that eye-tracking was the best for predicting the class Both, whereas interaction data was the best for predicting the classes None, Boredom and Frustration.

\cite{lalle2018prediction} works on predicting emotion-valence using eye-tracking data on a different MetaTutor study. The eye-tracker used in this study is the SMI RED 250. This dataset (dataset B onward) has valid eye-tracking data for 31 students with 123 EV reports and valid interaction data for 31 students with 176 EV reports. In this study, students were not asked a fixed number of EVs with the average EV count around 6.7 per student. Combining interaction and eye-tracking data with this dataset has not been done before.

For this RPE project, we will first extend the task of predicting co-occurring emotions using eye-tracking and interaction data in \cite{lalle2021predict} by combining dataset A and dataset B. We will then evaluate the efficacy of deep-learning and ensemble models for the task of predicting co-occurring emotions.

\section{Proposed Project}

The two user studies for dataset A and dataset B are structurally similar, so it makes sense to attempt to combine the two datasets, however, there are variations among the studies that make this task non-trivial. These include the time intervals between EV reports, the total number of EVs asked per student, and the type of eye-tracker used. The first part of this RPE project involves identifying these differences in detail and coming up with a way to combine the two datasets. Combining the two datasets would give us valid eye-tracking data for 92 students with 396 EV reports and valid interaction data for 96 students with 501 EV reports.


With the larger dataset (\textit{MetaTutor dataset} onward), the next step of the RPE project is to deploy machine-learning models to predict co-occurring emotions. Choosing the right pair(s) of co-occurring emotions for prediction tasks depends on the distribution of emotions reported in the MetaTutor dataset, which may be different from dataset A. We will look at 4-way classifications of an emotion pair just as in \cite{lalle2021predict}. We will use standard machine learning models such as random forest (RF), support-vector machine (SVM) and logistic regression (LR) classifiers as a baseline since they have been extensively used for affect detection \autocite{zeng2008survey} and have been used for the task of predicting co-occurring emotions \autocite{lalle2021predict}. We will implement and evaluate the performance of a fully-connected neural network against these baseline classifiers. This would be a first look at deep-learning models to target the prediction of co-occurring emotions in ITS.

Using interaction data and eye-tracking data has shown promise in affect detection \autocite{lalle2016predicting, lalle2021predict}. A limitation of \autocite{lalle2021predict} was that even though classifiers trained on either eye-tracking and interaction data worked well individually and complemented each other depending on the target-class, combining the two datasets through feature-fusion did not improve performance. So, we plan on investigating ensemble classifiers involving the standard machine-learning models trained on the different data sources, eye-tracking and interaction data. An ensemble model could leverage the fact that models trained on the two sources complement each other and have high class accuracies for different classes.

In \cite{sims2020neural}, user confusion is predicted using deep-learning models trained on raw sequences of eye-tracking data. Predictions are done in a different context of users interacting with ValueChart \autocite{carenini2004valuecharts}. The work features a Recurrent Neural-Network (RNN) trained on raw eye-tracking sequences, a Convolutional Neural-Network (CNN) trained on scan-paths, and an ensemble model, VTNet, combining the CNN and the RNN. The VTnet architecture in \cite{sims2020neural} works by connecting the outputs of the RNN and the CNN and forwarding them to a fully-connected neural network. They found that VTNet combines the strength of CNNs in spatial reasoning with the strength of RNNs in temporal reasoning and outperformed an existing RF model in literature \autocite{lalle2016predicting} in terms of accuracy.
The last part of the project involves investigating the feasibility of the VTnet architecture used in \cite{sims2020neural} on the MetaTutor dataset. This involves a novel upgrade to the VTnet architecture by including both eye-tracking and interaction data as inputs to the network, and assessing its performance against the other models. This would be the first work that combines eye-tracking and interaction data in a deep-learning model for affect prediction.

\section{Timeline}

A proposed timeline of the project will be as follows.

\begin{itemize}
    \item \textbf{May to Mid-June:} Identify differences between the two datasets and understand how to combine them.

    \item \textbf{Mid-June to Mid-July:} Evaluate performance of the fully-connected neural network and the ensemble models and to the baseline models.

    \item \textbf{Mid-July to End-August:} Upgrade VTnet architecture to include interaction data and assess its performance.

\end{itemize}

\printbibliography
\end{document}
